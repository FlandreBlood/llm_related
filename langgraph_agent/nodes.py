import json
import logging
from typing import Annotated, Literal
from langchain_core.messages import AIMessage, HumanMessage,  SystemMessage, ToolMessage
from langgraph.types import Command, interrupt
from langchain_openai import ChatOpenAI
from state import State
from prompts import *
from tools import *
# os.environ["DEEPSEEK_API_KEY"] = ""
from langchain_deepseek import ChatDeepSeek
# llm = ChatDeepSeek(model="deepseek-chat")
llm = ChatOpenAI(model="qwen-plus-2025-04-28", temperature=0.0, base_url='https://dashscope.aliyuncs.com/compatible-mode/v1', api_key='')
# llm = ChatOpenAI(model="deepseek-chat", temperature=0.0, base_url='https://api.deepseek.com', api_key='')

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
hander = logging.StreamHandler()
hander.setLevel(logging.INFO)

formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
hander.setFormatter(formatter)
logger.addHandler(hander)

def extract_json(text):
    if '```json' not in text:
        return text
    text = text.split('```json')[1].split('```')[0].strip()
    return text

def extract_answer(text):
    if '</think>' in text:
        answer = text.split("</think>")[-1]
        return answer.strip()
    
    return text

def create_planner_node(state: State):
    logger.info("***æ­£åœ¨è¿è¡ŒCreate Planner node***")
    messages = [SystemMessage(content=PLAN_SYSTEM_PROMPT), HumanMessage(content=PLAN_CREATE_PROMPT.format(user_message = state['user_message']))]
    response = llm.invoke(messages)
    response = response.model_dump_json(indent=4, exclude_none=True)
    response = json.loads(response)
    plan = json.loads(extract_json(extract_answer(response['content'])))
    state['messages'] += [AIMessage(content=json.dumps(plan, ensure_ascii=False))]
    return Command(goto="execute", update={"plan": plan})

def update_planner_node(state: State):
    logger.info("***æ­£åœ¨è¿è¡ŒUpdate Planner node***")
    plan = state['plan']
    goal = plan['goal']
    state['messages'].extend([SystemMessage(content=PLAN_SYSTEM_PROMPT), HumanMessage(content=UPDATE_PLAN_PROMPT.format(plan = plan, goal=goal))])
    messages = state['messages']
    while True:
        try:
            response = llm.invoke(messages)
            response = response.model_dump_json(indent=4, exclude_none=True)
            response = json.loads(response)
            plan = json.loads(extract_json(extract_answer(response['content'])))
            state['messages']+=[AIMessage(content=json.dumps(plan, ensure_ascii=False))]
            return Command(goto="execute", update={"plan": plan})
        except Exception as e:
            messages += [HumanMessage(content=f"jsonæ ¼å¼é”™è¯¯:{e}")]
    
def execute_node(state: State):
    """
    LangGraph æ‰§è¡ŒèŠ‚ç‚¹ï¼šè‡ªåŠ¨é€‰æ‹©å·¥å…·â†’æ‰§è¡Œâ†’åæ€ã€‚

    è®¾è®¡è¦ç‚¹ï¼ˆé’ˆå¯¹åŒæ­¥ graph.invoke è°ƒç”¨ï¼‰ï¼š
    1. **åæ€é˜¶æ®µæ£€æµ‹**ï¼šè‹¥ä¸Šä¸€è½®å·²æ’å…¥ AIMessage(tool_calls)+ToolMessageï¼Œåˆ™ç«‹å³è®©æ¨¡å‹åæ€ï¼›
       ç›´åˆ°åæ€å›å¤ä¸­ä¸å†åŒ…å« `tool_calls`ã€‚
    2. **åˆå§‹é˜¶æ®µ**ï¼šä¸ºå½“å‰å¾…åŠæ­¥éª¤æ„é€  System+Human æç¤ºï¼Œ
       ç¬¬ä¸€æ¬¡è°ƒç”¨æ¨¡å‹å†³å®šå·¥å…·ï¼›è‹¥æœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ™æ‰§è¡Œå¹¶æŠŠ ToolMessage
       ä¸ AIMessage(tool_calls) ä¸€å¹¶å†™å…¥ `state["messages"]`ï¼Œç„¶å returnï¼Œ
       ç­‰ä¸‹ä¸€è½®è¿›å…¥åæ€é˜¶æ®µã€‚
    3. ä»»ä½•æ—¶å€™åªè¦æ¨¡å‹å›å¤ä¸å¸¦ tool_callsï¼Œç›´æ¥æ ‡è®°æ­¥éª¤å®Œæˆã€‚

    "æ€è·¯å›¾"  
    åˆå§‹ â†’ AI(tool_calls) â†’ ToolMessage â†’ (è¿”å› execute)
       â†˜ æ—  tool_calls
          â†˜ æ ‡è®°å®Œæˆ / è¿›å…¥ä¸‹ä¸€æ­¥
    åæ€ â†’ AI(no tool_calls) â†’ æ ‡è®°å®Œæˆ / è¿›å…¥ä¸‹ä¸€æ­¥
    """

    # ============================= 0. è®°å½•å¯åŠ¨ =============================
    logger.info("***æ­£åœ¨è¿è¡Œexecute_node***")

    # å¤åˆ¶å†å²æ¶ˆæ¯ï¼Œåç»­å¯èƒ½ä¿®æ”¹
    messages = list(state.get("messages", []))

    # =====================================================================
    # 1. åæ€é˜¶æ®µï¼šè‹¥æœ«å°¾æ˜¯  AIMessage(tool_calls) + ToolMessageï¼Œå°±è®©æ¨¡å‹åæ€
    # =====================================================================
    if (
        len(messages) >= 2
        and isinstance(messages[-2], AIMessage)
        and messages[-2].additional_kwargs.get("tool_calls")
        and isinstance(messages[-1], ToolMessage)
    ):
        logger.info("ğŸ” è¿›å…¥å·¥å…·åæ€é˜¶æ®µ")

        # å¯èƒ½å¤šè½®åæ€ï¼šåªè¦æ¨¡å‹ç»§ç»­è¿”å› tool_callsï¼Œå°±ç»§ç»­æ‰§è¡Œå·¥å…·
        while True:
            # åæ€è°ƒç”¨å¿…é¡»ç»‘å®šå·¥å…·ï¼ˆæ¨¡å‹å¯èƒ½å†æ¬¡è°ƒç”¨ï¼‰
            MAX_REFLECT = 3 #æ¨¡å‹æœ€å¤šå°è¯•3æ¬¡
            tries = 0 # å°è¯•æ¬¡æ•°
            reflect_resp = llm.bind_tools([create_file, str_replace, shell_exec]).invoke(messages)
            logger.info(f"â™»ï¸ åæ€è¿”å›:\n{reflect_resp}")   # æŠŠåæ€å†…å®¹æ‰“å‡ºæ¥
            # è‹¥ä¸å†å¸¦ tool_callsï¼Œåˆ™åæ€ç»“æŸ
            if not reflect_resp.tool_calls:
                messages.append(reflect_resp)
                break
            tries += 1

            # è¶…è¿‡æ¬¡æ•°å°šæœªæˆåŠŸ
            if tries == MAX_REFLECT:
                logger.warning("âš ï¸ åæ€å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä»æœªæˆåŠŸï¼Œè·³è¿‡è¯¥æ­¥éª¤ã€‚")
                steps[idx]['status'] = 'skipped'

            # å¦åˆ™æ‰§è¡Œæ–°å·¥å…·ï¼Œå†æŠŠç»“æœä½œä¸º ToolMessage å†™å…¥
            tc = reflect_resp.tool_calls[0]
            tc_id, tname, targs = tc["id"], tc["name"], tc["args"]
            tool_map = {"create_file": create_file, "str_replace": str_replace, "shell_exec": shell_exec}
            tresult = tool_map[tname].invoke(targs) if tname in tool_map else {"error": "unknown tool"}
            tool_msg = ToolMessage(content=json.dumps(tresult, ensure_ascii=False), tool_call_id=tc_id)
            messages.extend([reflect_resp, tool_msg])  # ç»§ç»­å¾ªç¯åæ€
            logger.info("å·¥å…·è°ƒç”¨ JSON: %s", json.dumps(tc, ensure_ascii=False, indent=2))
            

        # ------- æ­¥éª¤æ ‡è®°ä¸ºå®Œæˆå¹¶å†™å› state -------
        plan = state.get("plan", {})
        steps = plan.get("steps", []) if isinstance(plan, dict) else plan
        idx = next((i for i, s in enumerate(steps) if s.get("status") == "pending"), None)
        if idx is not None:
            steps[idx]["status"] = "completed"
        # past_steps è®°å½•
        past_steps = state.get("past_steps", [])
        past_steps.append((f"æ­¥éª¤ {idx+1}: {steps[idx]['title']}", messages[-1].content))

        return Command(goto="execute", update={
            "plan": plan,
            "messages": messages,
            "past_steps": past_steps
        })

    # =====================================================================
    # 2. åˆå§‹é˜¶æ®µï¼šæ­£å¸¸é€‰æ‹©å·¥å…·å¹¶æ‰§è¡Œ
    # =====================================================================
    plan_obj = state.get("plan", {})
    steps = plan_obj.get("steps", []) if isinstance(plan_obj, dict) else plan_obj

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ª pending æ­¥éª¤
    cur_task, task_idx = None, -1
    for i, st in enumerate(steps):
        if st.get("status") == "pending":
            cur_task, task_idx = st, i
            break

    # æ‰€æœ‰æ­¥éª¤å®Œæˆ â†’ è·³åˆ° report
    if cur_task is None:
        logger.info("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œå‡†å¤‡ç”ŸæˆæŠ¥å‘Šã€‚")
        summary = "\n\n---\n\n".join([
            f"## {t}\n\n**ç»“æœ:**\n```\n{r}\n```" for t, r in state.get("past_steps", [])
        ])
        obs_msg = [HumanMessage(content="è¿™æ˜¯ä¹‹å‰æ‰€æœ‰æ­¥éª¤çš„æ‰§è¡Œæ‘˜è¦å’Œç»“æœï¼Œè¯·æ ¹æ®è¿™äº›ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼š\n\n"+summary)]
        return Command(goto="report", update={"observations": obs_msg})

    logger.info(f"å½“å‰æ‰§è¡ŒSTEP:{cur_task}")

    # -------- æ„é€  prompt --------
    if not any(isinstance(m, SystemMessage) and m.content == EXECUTE_SYSTEM_PROMPT for m in messages):
        messages.append(SystemMessage(content=EXECUTE_SYSTEM_PROMPT))

    messages.append(HumanMessage(content=EXECUTION_PROMPT.format(
        user_message=state.get('user_message', ''), step=cur_task)))

    # -------- ç¬¬ä¸€æ¬¡æ¨¡å‹è°ƒç”¨ï¼šé€‰æ‹©å¹¶è°ƒç”¨å·¥å…· --------
    first_resp = llm.bind_tools([create_file, str_replace, shell_exec]).invoke(messages)
    logger.info(f"ğŸ› ï¸ é¦–æ¬¡è°ƒç”¨è¿”å›:\n{first_resp}")

    # è‹¥æ¨¡å‹ç›´æ¥å®Œæˆï¼Œæ—  tool_calls
    if not first_resp.tool_calls:
        steps[task_idx]['status'] = 'completed'
        return Command(goto="execute", update={
            "plan": plan_obj,
            "messages": messages + [first_resp],
        })

    # ---- æ‰§è¡Œå·¥å…· ----
    tc = first_resp.tool_calls[0]
    tname, targs, tc_id = tc["name"], tc["args"], tc["id"]
    tool_map = {"create_file": create_file, "str_replace": str_replace, "shell_exec": shell_exec}
    tresult = tool_map[tname].invoke(targs) if tname in tool_map else {"error": "unknown tool"}
    logger.info(f"å·²æ‰§è¡Œå·¥å…· {tname}ï¼Œç»“æœ: {tresult}")

    # è®°å½• AI(tool_calls)+ToolMessageï¼Œå¹¶è¿”å›æ‰§è¡ŒèŠ‚ç‚¹ç­‰å¾…åæ€
    tool_msg = ToolMessage(content=json.dumps(tresult, ensure_ascii=False), tool_call_id=tc_id)
    messages.extend([first_resp, tool_msg])

    return Command(goto="execute", update={
        "plan": plan_obj,
        "messages": messages,
        "past_steps": state.get("past_steps", [])
    })


def report_node(state: State):
    """æ ¹æ®æ‰§è¡Œæ‘˜è¦ï¼Œç”Ÿæˆå¹¶ä¿å­˜æœ€ç»ˆæŠ¥å‘Š"""
    logger.info("***æ­£åœ¨è¿è¡Œreport_node***")
    
    # ä»çŠ¶æ€ä¸­è·å–ç”± execute_node å‡†å¤‡å¥½çš„ã€åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ observations
    observation_messages = state.get("observations", [HumanMessage(content="æ²¡æœ‰å¯ç”¨çš„è§‚å¯Ÿç»“æœã€‚")])

    # å‡†å¤‡è°ƒç”¨å¤§æ¨¡å‹æ‰€éœ€çš„æ¶ˆæ¯åˆ—è¡¨
    messages = observation_messages + [SystemMessage(content=REPORT_SYSTEM_PROMPT)]
    
    # è°ƒç”¨å¤§æ¨¡å‹ç”ŸæˆæŠ¥å‘Šå†…å®¹ï¼Œå®ƒå¯èƒ½ä¼šä½¿ç”¨ create_file å·¥å…·
    response = llm.bind_tools([create_file]).invoke(messages)

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¯·æ±‚è°ƒç”¨å·¥å…·ï¼ˆå³ä¿å­˜æ–‡ä»¶ï¼‰
    if response.tool_calls:
        tool_call = response.tool_calls[0]
        if tool_call.get("name") == create_file.name:
            logger.info("æ¨¡å‹è¯·æ±‚åˆ›å»ºæŠ¥å‘Šæ–‡ä»¶ã€‚")
            report_content = tool_call.get("args", {}).get("file_contents", "")
            tool_result = create_file.invoke(tool_call.get("args"))
            logger.info(f"tool_result: {tool_result}")
            final_report_content = report_content
            # ä¿®å¤å¯¹è¿”å›ç»“æ„çš„å¤„ç†
            if isinstance(tool_result.get("message"), str):
                logger.info("å·¥å…·ç»“æœ: %s", tool_result["message"])
            elif isinstance(tool_result.get("message"), dict):
                logger.info("å·¥å…·ç»“æœ stdoutâ–¼\n%s\nstderrâ–¼\n%s",
                            tool_result["message"].get("stdout", ""),
                            tool_result["message"].get("stderr", ""))
            else:
                logger.info("å·¥å…·è¿”å›æœªçŸ¥ç»“æ„: %s", tool_result)

            final_report_content = report_content
        else:
            final_report_content = f"æŠ¥å‘Šç”ŸæˆæœŸé—´å‡ºç°æ„å¤–çš„å·¥å…·è°ƒç”¨: {response.content}"
    else:
        logger.info("æ¨¡å‹ç›´æ¥è¿”å›äº†æŠ¥å‘Šå†…å®¹ã€‚å°†æ‰‹åŠ¨åˆ›å»ºæ–‡ä»¶")
        final_report_content = response.content
        create_file.invoke({"file_name": "final_analysis_report.md", "file_contents": final_report_content})
            
    return {"final_report": final_report_content}